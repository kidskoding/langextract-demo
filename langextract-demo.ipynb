{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangExtract: A Comprehensive Guide for Contributors\n",
    "\n",
    "This notebook walks you through **everything** you need to know about [langextract](https://github.com/SalML/LangExtract) — from basic usage to its internal architecture — so you can confidently contribute to the project.\n",
    "\n",
    "## What is LangExtract?\n",
    "\n",
    "LangExtract is a Python library for **structured information extraction** from text using Large Language Models (LLMs). Given some text, a description of what to extract, and a few examples, it:\n",
    "\n",
    "1. **Chunks** the text into manageable pieces\n",
    "2. **Prompts** an LLM with your examples (few-shot learning)\n",
    "3. **Parses** the LLM's structured output (JSON/YAML)\n",
    "4. **Aligns** extracted spans back to the original text (exact character positions)\n",
    "5. **Visualizes** results as interactive, color-coded HTML\n",
    "\n",
    "Think of it as: *\"I want to find all the characters, emotions, and relationships in this Shakespeare passage\"* — and LangExtract handles the entire pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Here's how data flows through the system:\n",
    "\n",
    "```\n",
    "User Input (text / URL / Documents)\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│  lx.extract()  — the main entry point       │\n",
    "│                                              │\n",
    "│  1. Model Factory  ─── creates LLM provider │\n",
    "│     ├─ Gemini, OpenAI, or Ollama             │\n",
    "│     └─ Applies schema constraints            │\n",
    "│                                              │\n",
    "│  2. Tokenizer  ─── splits text into tokens   │\n",
    "│                                              │\n",
    "│  3. Chunking  ─── splits into sized chunks   │\n",
    "│     (controlled by max_char_buffer)           │\n",
    "│                                              │\n",
    "│  4. Prompt Template  ─── builds few-shot     │\n",
    "│     prompts from your examples               │\n",
    "│                                              │\n",
    "│  5. LLM Inference  ─── calls the model       │\n",
    "│     (parallel batches via max_workers)        │\n",
    "│                                              │\n",
    "│  6. Resolver  ─── parses JSON/YAML output    │\n",
    "│     └─ Creates Extraction objects             │\n",
    "│                                              │\n",
    "│  7. Alignment  ─── maps extractions back     │\n",
    "│     to source text (char_interval)            │\n",
    "│                                              │\n",
    "│  8. Multi-pass Merging (if passes > 1)       │\n",
    "│     └─ Merges non-overlapping extractions    │\n",
    "└─────────────────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "AnnotatedDocument (text + extractions with positions)\n",
    "    │\n",
    "    ├──▶ lx.io.save_annotated_documents()  →  JSONL file\n",
    "    │\n",
    "    └──▶ lx.visualize()  →  Interactive HTML\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Modules (for contributors)\n",
    "\n",
    "| Module | What it does | Key classes/functions |\n",
    "|--------|-------------|----------------------|\n",
    "| `core/data.py` | Core data structures | `Extraction`, `Document`, `AnnotatedDocument`, `ExampleData`, `CharInterval`, `AlignmentStatus` |\n",
    "| `annotation.py` | Orchestrates the full extraction pipeline | `Annotator.annotate_text()`, `Annotator.annotate_documents()` |\n",
    "| `factory.py` | Creates LLM providers from model IDs | `create_model()`, `ModelConfig` |\n",
    "| `providers/` | LLM provider implementations | `GeminiLanguageModel`, `OpenAILanguageModel`, `OllamaLanguageModel` |\n",
    "| `prompting.py` | Builds few-shot prompts | `PromptTemplateStructured`, `QAPromptGenerator` |\n",
    "| `chunking.py` | Splits text into sized chunks | `TextChunk` |\n",
    "| `resolver.py` | Parses LLM output and aligns to text | `Resolver`, `WordAligner` |\n",
    "| `core/schema.py` | Structured output schemas | `BaseSchema`, `FormatModeSchema` |\n",
    "| `core/tokenizer.py` | Text tokenization | `Tokenizer`, `TokenInterval`, `TokenizedText` |\n",
    "| `io.py` | Save/load annotated documents | `save_annotated_documents()`, `load_annotated_documents_jsonl()` |\n",
    "| `visualization.py` | Interactive HTML visualization | `visualize()` |\n",
    "| `plugins.py` | Dynamic provider registration | Plugin entry points |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup\n",
    "\n",
    "First, import langextract and load your API key from a `.env` file.\n",
    "\n",
    "LangExtract resolves API keys from environment variables:\n",
    "- `GEMINI_API_KEY` for Gemini models\n",
    "- `OPENAI_API_KEY` for OpenAI models\n",
    "- `LANGEXTRACT_API_KEY` as a universal fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langextract as lx\n",
    "import textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Core Data Structures\n",
    "\n",
    "Before using `lx.extract()`, you need to understand the three fundamental data structures.\n",
    "\n",
    "### 2a. `Extraction` — a single extracted entity\n",
    "\n",
    "An `Extraction` represents one thing you found in the text:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Extraction:\n",
    "    extraction_class: str           # Category (e.g. \"character\", \"emotion\")\n",
    "    extraction_text: str            # The exact text span (e.g. \"ROMEO\")\n",
    "    attributes: dict | None         # Extra metadata (e.g. {\"mood\": \"happy\"})\n",
    "    char_interval: CharInterval     # Start/end positions in the source text\n",
    "    alignment_status: AlignmentStatus  # How well it matched (EXACT, FUZZY, etc.)\n",
    "```\n",
    "\n",
    "**`AlignmentStatus` values:**\n",
    "- `MATCH_EXACT` — perfect token-level match\n",
    "- `MATCH_FUZZY` — fuzzy overlap match (configurable threshold, default 0.75)\n",
    "- `MATCH_GREATER` — matched text is longer than the extraction\n",
    "- `MATCH_LESSER` — partial exact match\n",
    "\n",
    "### 2b. `ExampleData` — a few-shot training example\n",
    "\n",
    "You teach the LLM *what* to extract by providing examples:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class ExampleData:\n",
    "    text: str                        # The example text\n",
    "    extractions: list[Extraction]    # What should be extracted from it\n",
    "```\n",
    "\n",
    "### 2c. `AnnotatedDocument` — the output\n",
    "\n",
    "After extraction, you get back an `AnnotatedDocument`:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class AnnotatedDocument:\n",
    "    text: str                        # The original input text\n",
    "    extractions: list[Extraction]    # All extractions found\n",
    "    document_id: str                 # Auto-generated unique ID\n",
    "```\n",
    "\n",
    "Each extraction in the result has `char_interval` set, so you know *exactly* where in the text it was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Defining Your Extraction Task\n",
    "\n",
    "You need two things:\n",
    "1. **A prompt** — tells the LLM what kinds of entities to extract\n",
    "2. **Examples** — shows the LLM the expected output format (few-shot learning)\n",
    "\n",
    "### The Prompt\n",
    "\n",
    "The prompt describes your extraction task in natural language. Tips:\n",
    "- Be specific about what categories to extract\n",
    "- Tell it to use exact text (not paraphrase)\n",
    "- Mention that attributes should add meaningful context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract characters, emotions, and relationships in order of appearance.\n",
      "Use exact text for extractions. Do not paraphrase or overlap entities.\n",
      "Provide meaningful attributes for each entity to add context.\n"
     ]
    }
   ],
   "source": [
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract characters, emotions, and relationships in order of appearance.\n",
    "    Use exact text for extractions. Do not paraphrase or overlap entities.\n",
    "    Provide meaningful attributes for each entity to add context.\"\"\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Examples (Few-Shot Learning)\n",
    "\n",
    "Examples are **critical** — they teach the LLM:\n",
    "- What `extraction_class` values to use (e.g. \"character\", \"emotion\", \"relationship\")\n",
    "- What text spans to extract (`extraction_text` must be an exact substring)\n",
    "- What attributes to include and how to format them\n",
    "\n",
    "LangExtract uses these examples to:\n",
    "1. Build the few-shot prompt sent to the LLM\n",
    "2. Generate structured output schemas (if `use_schema_constraints=True`)\n",
    "3. Validate prompt alignment (catches mistakes early)\n",
    "\n",
    "**Important:** `extraction_text` must be an **exact substring** of the example `text`. LangExtract validates this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text: 'ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.'\n",
      "Number of extractions in example: 3\n",
      "  [character] \"ROMEO\" -> {'emotional_state': 'wonder'}\n",
      "  [emotion] \"But soft!\" -> {'feeling': 'gentle awe'}\n",
      "  [relationship] \"Juliet is the sun\" -> {'type': 'metaphor'}\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",\n",
    "        extractions=[\n",
    "            # Category: \"character\" — we extract the character's name\n",
    "            # extraction_text MUST appear exactly in the example text\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"ROMEO\",\n",
    "                attributes={\"emotional_state\": \"wonder\"}\n",
    "            ),\n",
    "            # Category: \"emotion\" — we extract the phrase that conveys emotion\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"But soft!\",\n",
    "                attributes={\"feeling\": \"gentle awe\"}\n",
    "            ),\n",
    "            # Category: \"relationship\" — we extract the phrase describing a relationship\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"relationship\",\n",
    "                extraction_text=\"Juliet is the sun\",\n",
    "                attributes={\"type\": \"metaphor\"}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Example text: {examples[0].text!r}\")\n",
    "print(f\"Number of extractions in example: {len(examples[0].extractions)}\")\n",
    "for ext in examples[0].extractions:\n",
    "    print(f\"  [{ext.extraction_class}] \\\"{ext.extraction_text}\\\" -> {ext.attributes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Running Extraction with `lx.extract()`\n",
    "\n",
    "This is the main entry point. Here's what each parameter does:\n",
    "\n",
    "| Parameter | Type | Default | What it does |\n",
    "|-----------|------|---------|-------------|\n",
    "| `text_or_documents` | `str`, URL, or `Iterable[Document]` | *required* | The text to extract from |\n",
    "| `prompt_description` | `str` | `None` | Natural language description of your task |\n",
    "| `examples` | `list[ExampleData]` | `None` | Few-shot examples (**required**) |\n",
    "| `model_id` | `str` | `\"gemini-2.5-flash\"` | Which LLM to use |\n",
    "| `max_char_buffer` | `int` | `1000` | Max chars per LLM call (controls chunking) |\n",
    "| `temperature` | `float` | `None` | LLM temperature (lower = more deterministic) |\n",
    "| `use_schema_constraints` | `bool` | `True` | Generate structured output schema from examples |\n",
    "| `batch_length` | `int` | `10` | Chunks per batch |\n",
    "| `max_workers` | `int` | `10` | Parallel workers for inference |\n",
    "| `extraction_passes` | `int` | `1` | Run multiple passes for better recall |\n",
    "| `format_type` | `FormatType` | `None` | Force JSON or YAML output format |\n",
    "| `debug` | `bool` | `False` | Enable debug logging |\n",
    "\n",
    "**Returns:**\n",
    "- `AnnotatedDocument` if input is a single string/URL\n",
    "- `list[AnnotatedDocument]` if input is multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m, current=\u001b[92m68\u001b[0m chars, processed=\u001b[92m0\u001b[0m chars:  [00:02]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"\n",
    "\n",
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",  # also supports: gpt-4o, llama3, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Result\n",
    "\n",
    "The result is an `AnnotatedDocument`. Let's look at what was extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'Lady Juliet gazed longingly at the stars, her heart aching for Romeo'\n",
      "Document ID: doc_f223e933\n",
      "Number of extractions: 3\n",
      "\n",
      "Extraction 1:\n",
      "  Class:      character\n",
      "  Text:       \"Lady Juliet\"\n",
      "  Attributes: {'emotional_state': 'longing'}\n",
      "  Position:   chars 0-11\n",
      "  Alignment:  AlignmentStatus.MATCH_EXACT\n",
      "  Verified:   \"Lady Juliet\"\n",
      "\n",
      "Extraction 2:\n",
      "  Class:      emotion\n",
      "  Text:       \"aching\"\n",
      "  Attributes: {'feeling': 'painful longing'}\n",
      "  Position:   chars 52-58\n",
      "  Alignment:  AlignmentStatus.MATCH_EXACT\n",
      "  Verified:   \"aching\"\n",
      "\n",
      "Extraction 3:\n",
      "  Class:      relationship\n",
      "  Text:       \"for Romeo\"\n",
      "  Attributes: {'type': 'romantic longing'}\n",
      "  Position:   chars 59-68\n",
      "  Alignment:  AlignmentStatus.MATCH_EXACT\n",
      "  Verified:   \"for Romeo\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: {result.text!r}\")\n",
    "print(f\"Document ID: {result.document_id}\")\n",
    "print(f\"Number of extractions: {len(result.extractions)}\")\n",
    "print()\n",
    "\n",
    "for i, ext in enumerate(result.extractions):\n",
    "    print(f\"Extraction {i+1}:\")\n",
    "    print(f\"  Class:      {ext.extraction_class}\")\n",
    "    print(f\"  Text:       \\\"{ext.extraction_text}\\\"\")\n",
    "    print(f\"  Attributes: {ext.attributes}\")\n",
    "    print(f\"  Position:   chars {ext.char_interval.start_pos}-{ext.char_interval.end_pos}\")\n",
    "    print(f\"  Alignment:  {ext.alignment_status}\")\n",
    "    # Verify the char_interval points to the right text\n",
    "    if ext.char_interval.start_pos is not None:\n",
    "        actual = result.text[ext.char_interval.start_pos:ext.char_interval.end_pos]\n",
    "        print(f\"  Verified:   \\\"{actual}\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Saving Results with `lx.io`\n",
    "\n",
    "LangExtract saves results as **JSON Lines** (`.jsonl`) — one JSON object per line, one document per line.\n",
    "\n",
    "This format is:\n",
    "- Easy to stream (read one line at a time)\n",
    "- Easy to append to\n",
    "- Compatible with most data tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mextraction_results.jsonl\u001b[0m: 1 docs [00:00, 1085.76 docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mextraction_results.jsonl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save to a JSONL file\n",
    "lx.io.save_annotated_documents(\n",
    "    [result],                           # list of AnnotatedDocument(s)\n",
    "    output_name=\"extraction_results.jsonl\",\n",
    "    output_dir=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# You can load them back later\u001b[39;00m\n\u001b[32m      2\u001b[39m loaded_docs = lx.io.load_annotated_documents_jsonl(\u001b[33m\"\u001b[39m\u001b[33mextraction_results.jsonl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloaded_docs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m document(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst doc has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loaded_docs[\u001b[32m0\u001b[39m].extractions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m extractions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "# You can load them back later\n",
    "loaded_docs = lx.io.load_annotated_documents_jsonl(\"extraction_results.jsonl\")\n",
    "print(f\"Loaded {len(loaded_docs)} document(s)\")\n",
    "print(f\"First doc has {len(loaded_docs[0].extractions)} extractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualization with `lx.visualize()`\n",
    "\n",
    "`lx.visualize()` creates an **interactive HTML widget** that:\n",
    "- Color-codes each extraction class (character = blue, emotion = green, etc.)\n",
    "- Highlights the extracted span in the text\n",
    "- Shows attributes on hover\n",
    "- Has play/pause controls to step through extractions one by one\n",
    "\n",
    "It automatically assigns colors from a 10-color palette:\n",
    "```\n",
    "Light Blue, Light Green, Light Yellow, Light Red, Light Orange,\n",
    "Light Purple, Light Teal, Light Pink, Light Grey, Pale Cyan\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `data_source`: an `AnnotatedDocument` or path to a `.jsonl` file\n",
    "- `animation_speed`: seconds between extractions during auto-play (default: 1.0)\n",
    "- `show_legend`: show color legend (default: True)\n",
    "- `gif_optimized`: larger fonts for screen recording (default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize directly from the JSONL file\n",
    "html_content = lx.visualize(\"extraction_results.jsonl\")\n",
    "\n",
    "# In Jupyter, this renders inline automatically.\n",
    "# To also save as a standalone HTML file:\n",
    "with open(\"visualization.html\", \"w\") as f:\n",
    "    if hasattr(html_content, 'data'):\n",
    "        f.write(html_content.data)\n",
    "    else:\n",
    "        f.write(html_content)\n",
    "\n",
    "html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Supported Models\n",
    "\n",
    "LangExtract supports three provider families. The `model_id` string is auto-routed to the correct provider.\n",
    "\n",
    "| Provider | Model IDs | API Key Env Var | Structured Output |\n",
    "|----------|-----------|-----------------|-------------------|\n",
    "| **Gemini** | `gemini-2.5-flash`, `gemini-2.5-pro`, etc. | `GEMINI_API_KEY` | Yes (JSON schema) |\n",
    "| **OpenAI** | `gpt-4o`, `gpt-4o-mini`, `gpt-5`, etc. | `OPENAI_API_KEY` | No |\n",
    "| **Ollama** (local) | `llama3`, `mistral`, `gemma`, `phi`, `qwen`, `deepseek`, etc. | N/A (runs locally) | No |\n",
    "\n",
    "You can also register custom providers via the **plugin system** (entry points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: A Different Example — Custom Categories\n",
    "\n",
    "Let's try a completely different extraction task to show how flexible LangExtract is. This time we'll extract **people, places, and activities** from casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m, current=\u001b[92m93\u001b[0m chars, processed=\u001b[92m0\u001b[0m chars:  [00:01]\n"
     ]
    }
   ],
   "source": [
    "custom_prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract people, places, and activities mentioned in the text.\n",
    "    Use exact text for extractions. Do not paraphrase.\n",
    "    Provide meaningful attributes for each entity to add context.\n",
    "    Do not skip any. Provide attributes for all extractions\"\"\")\n",
    "\n",
    "custom_examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"Sarah went to the library to study for her final exams.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"person\",\n",
    "                extraction_text=\"Sarah\",\n",
    "                attributes={\"role\": \"student\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"place\",\n",
    "                extraction_text=\"the library\",\n",
    "                attributes={\"type\": \"academic facility\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"activity\",\n",
    "                extraction_text=\"study for her final exams\",\n",
    "                attributes={\"purpose\": \"exam preparation\"}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "custom_input = \"Anirudh loves to go to UIUC and he enjoys spending time with his friends and having good food\"\n",
    "\n",
    "custom_result = lx.extract(\n",
    "    text_or_documents=custom_input,\n",
    "    prompt_description=custom_prompt,\n",
    "    examples=custom_examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Anirudh loves to go to UIUC and he enjoys spending time with his friends and having good food'\n",
      "\n",
      "  [person] \"Anirudh\" -> {'role': 'individual'}\n",
      "  [place] \"UIUC\" -> {'type': 'university'}\n",
      "  [activity] \"spending time with his friends\" -> {'purpose': 'socializing'}\n",
      "  [activity] \"having good food\" -> {'purpose': 'enjoyment'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input: {custom_result.text!r}\")\n",
    "print()\n",
    "for ext in custom_result.extractions:\n",
    "    print(f\"  [{ext.extraction_class}] \\\"{ext.extraction_text}\\\" -> {ext.attributes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}\n",
       ".lx-highlight .lx-tooltip {\n",
       "  visibility: hidden;\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s ease-in-out;\n",
       "  background: #333;\n",
       "  color: #fff;\n",
       "  text-align: left;\n",
       "  border-radius: 4px;\n",
       "  padding: 6px 8px;\n",
       "  position: absolute;\n",
       "  z-index: 1000;\n",
       "  bottom: 125%;\n",
       "  left: 50%;\n",
       "  transform: translateX(-50%);\n",
       "  font-size: 12px;\n",
       "  max-width: 240px;\n",
       "  white-space: normal;\n",
       "  box-shadow: 0 2px 6px rgba(0,0,0,0.3);\n",
       "}\n",
       ".lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }\n",
       ".lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }\n",
       ".lx-controls {\n",
       "  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;\n",
       "  padding: 12px; margin-bottom: 16px;\n",
       "}\n",
       ".lx-button-row {\n",
       "  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;\n",
       "}\n",
       ".lx-control-btn {\n",
       "  background: #4285f4; color: white; border: none; border-radius: 4px;\n",
       "  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;\n",
       "  transition: background-color 0.2s;\n",
       "}\n",
       ".lx-control-btn:hover { background: #3367d6; }\n",
       ".lx-progress-container {\n",
       "  margin-bottom: 8px;\n",
       "}\n",
       ".lx-progress-slider {\n",
       "  width: 100%; margin: 0; appearance: none; height: 6px;\n",
       "  background: #ddd; border-radius: 3px; outline: none;\n",
       "}\n",
       ".lx-progress-slider::-webkit-slider-thumb {\n",
       "  appearance: none; width: 18px; height: 18px; background: #4285f4;\n",
       "  border-radius: 50%; cursor: pointer;\n",
       "}\n",
       ".lx-progress-slider::-moz-range-thumb {\n",
       "  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;\n",
       "  cursor: pointer; border: none;\n",
       "}\n",
       ".lx-status-text {\n",
       "  text-align: center; font-size: 12px; color: #666; margin-top: 4px;\n",
       "}\n",
       ".lx-text-window {\n",
       "  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;\n",
       "  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;\n",
       "  line-height: 1.6;\n",
       "}\n",
       ".lx-attributes-panel {\n",
       "  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;\n",
       "  padding: 8px 10px; margin-top: 8px; font-size: 13px;\n",
       "}\n",
       ".lx-current-highlight {\n",
       "  border-bottom: 4px solid #ff4444;\n",
       "  font-weight: bold;\n",
       "  animation: lx-pulse 1s ease-in-out;\n",
       "}\n",
       "@keyframes lx-pulse {\n",
       "  0% { text-decoration-color: #ff4444; }\n",
       "  50% { text-decoration-color: #ff0000; }\n",
       "  100% { text-decoration-color: #ff4444; }\n",
       "}\n",
       ".lx-legend {\n",
       "  font-size: 12px; margin-bottom: 8px;\n",
       "  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;\n",
       "}\n",
       ".lx-label {\n",
       "  display: inline-block;\n",
       "  padding: 2px 4px;\n",
       "  border-radius: 3px;\n",
       "  margin-right: 4px;\n",
       "  color: #000;\n",
       "}\n",
       ".lx-attr-key {\n",
       "  font-weight: 600;\n",
       "  color: #1565c0;\n",
       "  letter-spacing: 0.3px;\n",
       "}\n",
       ".lx-attr-value {\n",
       "  font-weight: 400;\n",
       "  opacity: 0.85;\n",
       "  letter-spacing: 0.2px;\n",
       "}\n",
       "\n",
       "/* Add optimizations with larger fonts and better readability for GIFs */\n",
       ".lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }\n",
       ".lx-gif-optimized .lx-attributes-panel { font-size: 15px; }\n",
       ".lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }\n",
       "</style>\n",
       "<div class=\"lx-animated-wrapper lx-gif-optimized\">\n",
       "  <div class=\"lx-attributes-panel\">\n",
       "    <div class=\"lx-legend\">Highlights Legend: <span class=\"lx-label\" style=\"background-color:#D2E3FC;\">activity</span> <span class=\"lx-label\" style=\"background-color:#C8E6C9;\">person</span> <span class=\"lx-label\" style=\"background-color:#FEF0C3;\">place</span></div>\n",
       "    <div id=\"attributesContainer\"></div>\n",
       "  </div>\n",
       "  <div class=\"lx-text-window\" id=\"textWindow\">\n",
       "    <span class=\"lx-highlight lx-current-highlight\" data-idx=\"0\" style=\"background-color:#C8E6C9;\">Anirudh</span> loves to go to <span class=\"lx-highlight\" data-idx=\"1\" style=\"background-color:#FEF0C3;\">UIUC</span> and he enjoys <span class=\"lx-highlight\" data-idx=\"2\" style=\"background-color:#D2E3FC;\">spending time with his friends</span> and <span class=\"lx-highlight\" data-idx=\"3\" style=\"background-color:#D2E3FC;\">having good food</span>\n",
       "  </div>\n",
       "  <div class=\"lx-controls\">\n",
       "    <div class=\"lx-button-row\">\n",
       "      <button class=\"lx-control-btn\" onclick=\"playPause()\">▶️ Play</button>\n",
       "      <button class=\"lx-control-btn\" onclick=\"prevExtraction()\">⏮ Previous</button>\n",
       "      <button class=\"lx-control-btn\" onclick=\"nextExtraction()\">⏭ Next</button>\n",
       "    </div>\n",
       "    <div class=\"lx-progress-container\">\n",
       "      <input type=\"range\" id=\"progressSlider\" class=\"lx-progress-slider\"\n",
       "             min=\"0\" max=\"3\" value=\"0\"\n",
       "             onchange=\"jumpToExtraction(this.value)\">\n",
       "    </div>\n",
       "    <div class=\"lx-status-text\">\n",
       "      Entity <span id=\"entityInfo\">1/4</span> |\n",
       "      Pos <span id=\"posInfo\">[0-7]</span>\n",
       "    </div>\n",
       "  </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "  (function() {\n",
       "    const extractions = [{\"index\": 0, \"class\": \"person\", \"text\": \"Anirudh\", \"color\": \"#C8E6C9\", \"startPos\": 0, \"endPos\": 7, \"beforeText\": \"\", \"extractionText\": \"Anirudh\", \"afterText\": \" loves to go to UIUC and he enjoys spending time with his friends and having good food\", \"attributesHtml\": \"<div><strong>class:</strong> person</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">role</span>: <span class=\\\"lx-attr-value\\\">individual</span>}</div>\"}, {\"index\": 1, \"class\": \"place\", \"text\": \"UIUC\", \"color\": \"#FEF0C3\", \"startPos\": 23, \"endPos\": 27, \"beforeText\": \"Anirudh loves to go to \", \"extractionText\": \"UIUC\", \"afterText\": \" and he enjoys spending time with his friends and having good food\", \"attributesHtml\": \"<div><strong>class:</strong> place</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">type</span>: <span class=\\\"lx-attr-value\\\">university</span>}</div>\"}, {\"index\": 2, \"class\": \"activity\", \"text\": \"spending time with his friends\", \"color\": \"#D2E3FC\", \"startPos\": 42, \"endPos\": 72, \"beforeText\": \"Anirudh loves to go to UIUC and he enjoys \", \"extractionText\": \"spending time with his friends\", \"afterText\": \" and having good food\", \"attributesHtml\": \"<div><strong>class:</strong> activity</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">purpose</span>: <span class=\\\"lx-attr-value\\\">socializing</span>}</div>\"}, {\"index\": 3, \"class\": \"activity\", \"text\": \"having good food\", \"color\": \"#D2E3FC\", \"startPos\": 77, \"endPos\": 93, \"beforeText\": \"Anirudh loves to go to UIUC and he enjoys spending time with his friends and \", \"extractionText\": \"having good food\", \"afterText\": \"\", \"attributesHtml\": \"<div><strong>class:</strong> activity</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">purpose</span>: <span class=\\\"lx-attr-value\\\">enjoyment</span>}</div>\"}];\n",
       "    let currentIndex = 0;\n",
       "    let isPlaying = false;\n",
       "    let animationInterval = null;\n",
       "    let animationSpeed = 1.0;\n",
       "\n",
       "    function updateDisplay() {\n",
       "      const extraction = extractions[currentIndex];\n",
       "      if (!extraction) return;\n",
       "\n",
       "      document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;\n",
       "      document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;\n",
       "      document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';\n",
       "      document.getElementById('progressSlider').value = currentIndex;\n",
       "\n",
       "      const playBtn = document.querySelector('.lx-control-btn');\n",
       "      if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';\n",
       "\n",
       "      const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');\n",
       "      if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');\n",
       "      const currentSpan = document.querySelector('.lx-text-window span[data-idx=\"' + currentIndex + '\"]');\n",
       "      if (currentSpan) {\n",
       "        currentSpan.classList.add('lx-current-highlight');\n",
       "        currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function nextExtraction() {\n",
       "      currentIndex = (currentIndex + 1) % extractions.length;\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    function prevExtraction() {\n",
       "      currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    function jumpToExtraction(index) {\n",
       "      currentIndex = parseInt(index);\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    function playPause() {\n",
       "      if (isPlaying) {\n",
       "        clearInterval(animationInterval);\n",
       "        isPlaying = false;\n",
       "      } else {\n",
       "        animationInterval = setInterval(nextExtraction, animationSpeed * 1000);\n",
       "        isPlaying = true;\n",
       "      }\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    window.playPause = playPause;\n",
       "    window.nextExtraction = nextExtraction;\n",
       "    window.prevExtraction = prevExtraction;\n",
       "    window.jumpToExtraction = jumpToExtraction;\n",
       "\n",
       "    updateDisplay();\n",
       "  })();\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize this result directly from the AnnotatedDocument (no need to save first)\n",
    "lx.visualize(custom_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Advanced Features\n",
    "\n",
    "### 9a. Multi-pass Extraction (better recall)\n",
    "\n",
    "Sometimes the LLM misses entities on the first pass. Setting `extraction_passes > 1` runs extraction multiple times and merges the results. Overlapping extractions use a first-pass-wins strategy.\n",
    "\n",
    "```python\n",
    "result = lx.extract(\n",
    "    text_or_documents=long_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    extraction_passes=3,  # run 3 passes, merge results\n",
    ")\n",
    "```\n",
    "\n",
    "### 9b. Schema Constraints (more reliable output)\n",
    "\n",
    "When `use_schema_constraints=True` (the default), LangExtract generates a JSON schema from your examples and passes it to the LLM. This forces the model to return well-structured output. Currently best supported by Gemini.\n",
    "\n",
    "### 9c. Chunking Control\n",
    "\n",
    "`max_char_buffer` controls how much text is sent per LLM call. For long documents:\n",
    "- Smaller buffer = more API calls, but each is easier for the LLM\n",
    "- Larger buffer = fewer API calls, but may lose detail\n",
    "\n",
    "### 9d. Parallel Processing\n",
    "\n",
    "`batch_length` and `max_workers` control parallelism:\n",
    "- `batch_length`: how many chunks per batch\n",
    "- `max_workers`: max parallel API calls\n",
    "- Effective parallelism = min(batch_length, max_workers)\n",
    "\n",
    "### 9e. Processing URLs\n",
    "\n",
    "You can pass a URL directly as input — LangExtract will download and extract from it:\n",
    "\n",
    "```python\n",
    "result = lx.extract(\n",
    "    text_or_documents=\"https://example.com/article.txt\",\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")\n",
    "```\n",
    "\n",
    "### 9f. Batch Processing Multiple Documents\n",
    "\n",
    "To process many documents at once, wrap them in `Document` objects:\n",
    "\n",
    "```python\n",
    "docs = [\n",
    "    lx.data.Document(text=\"First document text...\"),\n",
    "    lx.data.Document(text=\"Second document text...\"),\n",
    "]\n",
    "\n",
    "results = lx.extract(\n",
    "    text_or_documents=docs,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")  # returns list[AnnotatedDocument]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Internals Deep Dive (for Contributors)\n",
    "\n",
    "If you want to contribute to langextract, here's how the key internal components work:\n",
    "\n",
    "### The Extraction Pipeline (step by step)\n",
    "\n",
    "**Step 1: Model Creation** (`factory.py`)\n",
    "- `factory.create_model(config)` matches `model_id` to a provider using regex patterns\n",
    "- Each provider (Gemini, OpenAI, Ollama) implements `BaseLanguageModel` with an `infer()` method\n",
    "- Schema constraints are applied via `apply_schema()` if supported\n",
    "\n",
    "**Step 2: Tokenization** (`core/tokenizer.py`)\n",
    "- Text is tokenized using a regex-based tokenizer\n",
    "- `TokenizedText` is lazy-computed and cached on the `Document` object\n",
    "- Each token has a `TokenInterval` tracking its position\n",
    "\n",
    "**Step 3: Chunking** (`chunking.py`)\n",
    "- Text is split into `TextChunk` objects, each ≤ `max_char_buffer` characters\n",
    "- Chunks include overlap to avoid splitting entities at boundaries\n",
    "- Each chunk carries its `TokenInterval` for later alignment\n",
    "\n",
    "**Step 4: Prompt Building** (`prompting.py`)\n",
    "- `PromptTemplateStructured` combines your prompt description + examples into a few-shot prompt\n",
    "- `QAPromptGenerator` formats each example as a Q&A pair\n",
    "- The chunk text is inserted as the \"question\" the LLM should answer\n",
    "\n",
    "**Step 5: Inference** (`annotation.py` + `providers/`)\n",
    "- The `Annotator` sends chunks to the LLM in parallel batches\n",
    "- Each provider's `infer()` method handles the actual API call\n",
    "- Progress bars show processing status\n",
    "\n",
    "**Step 6: Resolution** (`resolver.py`)\n",
    "- `Resolver.resolve()` parses the LLM's JSON/YAML output\n",
    "- Extracts code fences (` ```json...``` `) if present\n",
    "- Creates `Extraction` objects from the parsed data\n",
    "\n",
    "**Step 7: Alignment** (`resolver.py`)\n",
    "- `WordAligner.align()` maps each extraction back to the source text\n",
    "- Tries exact token matching first → `MATCH_EXACT`\n",
    "- Falls back to fuzzy matching if needed → `MATCH_FUZZY` (threshold 0.75)\n",
    "- Sets `char_interval` (start/end character positions) on each extraction\n",
    "- Sets `alignment_status` so you know how confident the match is\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "Key exception classes in `exceptions.py`:\n",
    "- `LangExtractError` — base class for all errors\n",
    "- `InvalidDatasetError` — empty or invalid input dataset\n",
    "- `InferenceConfigError` — model creation or config failure\n",
    "- `PromptAlignmentError` — example text doesn't contain extraction_text\n",
    "- `PromptBuilderError` — prompt template construction failure\n",
    "- `ParseError` — template parsing failure\n",
    "- `TokenUtilError` — tokenization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: How to Contribute\n",
    "\n",
    "Now that you understand the architecture, here are some areas where contributions are valuable:\n",
    "\n",
    "1. **New Providers** — Add support for more LLMs (Anthropic, Cohere, etc.) by implementing `BaseLanguageModel`\n",
    "2. **Schema Support** — Extend structured output schemas to more providers (currently best in Gemini)\n",
    "3. **Resolver Improvements** — Better parsing and alignment strategies\n",
    "4. **Visualization** — New visualization modes, better interactivity\n",
    "5. **Chunking Strategies** — Smarter text splitting (e.g., sentence-aware, paragraph-aware)\n",
    "6. **Testing** — More unit tests, edge case coverage\n",
    "7. **Documentation** — Examples, tutorials, API docs\n",
    "\n",
    "### Adding a New Provider (quick guide)\n",
    "\n",
    "1. Create a new file in `providers/` (e.g., `anthropic.py`)\n",
    "2. Implement a class that extends `BaseLanguageModel`\n",
    "3. Implement at minimum: `infer(prompt: str) -> str`\n",
    "4. Register it in the router with regex patterns for matching model IDs\n",
    "5. Or use the plugin system via entry points for external packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "import langextract as lx\n",
    "\n",
    "# 1. Define what to extract\n",
    "prompt = \"Extract characters and emotions. Use exact text.\"\n",
    "\n",
    "# 2. Provide examples\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"...\",\n",
    "        extractions=[lx.data.Extraction(extraction_class=\"...\", extraction_text=\"...\", attributes={...})]\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Extract\n",
    "result = lx.extract(text_or_documents=\"...\", prompt_description=prompt, examples=examples, model_id=\"gemini-2.5-flash\")\n",
    "\n",
    "# 4. Save\n",
    "lx.io.save_annotated_documents([result], output_name=\"output.jsonl\", output_dir=\".\")\n",
    "\n",
    "# 5. Visualize\n",
    "lx.visualize(\"output.jsonl\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
