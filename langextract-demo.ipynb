{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangExtract: A Comprehensive Guide for Contributors\n",
    "\n",
    "This notebook walks you through **everything** you need to know about [langextract](https://github.com/SalML/LangExtract) — from basic usage to its internal architecture — so you can confidently contribute to the project.\n",
    "\n",
    "## What is LangExtract?\n",
    "\n",
    "LangExtract is a Python library for **structured information extraction** from text using Large Language Models (LLMs). Given some text, a description of what to extract, and a few examples, it:\n",
    "\n",
    "1. **Chunks** the text into manageable pieces\n",
    "2. **Prompts** an LLM with your examples (few-shot learning)\n",
    "3. **Parses** the LLM's structured output (JSON/YAML)\n",
    "4. **Aligns** extracted spans back to the original text (exact character positions)\n",
    "5. **Visualizes** results as interactive, color-coded HTML\n",
    "\n",
    "Think of it as: *\"I want to find all the characters, emotions, and relationships in this Shakespeare passage\"* — and LangExtract handles the entire pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "Here's how data flows through the system:\n",
    "\n",
    "```\n",
    "User Input (text / URL / Documents)\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│  lx.extract()  — the main entry point       │\n",
    "│                                              │\n",
    "│  1. Model Factory  ─── creates LLM provider │\n",
    "│     ├─ Gemini, OpenAI, or Ollama             │\n",
    "│     └─ Applies schema constraints            │\n",
    "│                                              │\n",
    "│  2. Tokenizer  ─── splits text into tokens   │\n",
    "│                                              │\n",
    "│  3. Chunking  ─── splits into sized chunks   │\n",
    "│     (controlled by max_char_buffer)           │\n",
    "│                                              │\n",
    "│  4. Prompt Template  ─── builds few-shot     │\n",
    "│     prompts from your examples               │\n",
    "│                                              │\n",
    "│  5. LLM Inference  ─── calls the model       │\n",
    "│     (parallel batches via max_workers)        │\n",
    "│                                              │\n",
    "│  6. Resolver  ─── parses JSON/YAML output    │\n",
    "│     └─ Creates Extraction objects             │\n",
    "│                                              │\n",
    "│  7. Alignment  ─── maps extractions back     │\n",
    "│     to source text (char_interval)            │\n",
    "│                                              │\n",
    "│  8. Multi-pass Merging (if passes > 1)       │\n",
    "│     └─ Merges non-overlapping extractions    │\n",
    "└─────────────────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "AnnotatedDocument (text + extractions with positions)\n",
    "    │\n",
    "    ├──▶ lx.io.save_annotated_documents()  →  JSONL file\n",
    "    │\n",
    "    └──▶ lx.visualize()  →  Interactive HTML\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Modules (for contributors)\n",
    "\n",
    "| Module | What it does | Key classes/functions |\n",
    "|--------|-------------|----------------------|\n",
    "| `core/data.py` | Core data structures | `Extraction`, `Document`, `AnnotatedDocument`, `ExampleData`, `CharInterval`, `AlignmentStatus` |\n",
    "| `annotation.py` | Orchestrates the full extraction pipeline | `Annotator.annotate_text()`, `Annotator.annotate_documents()` |\n",
    "| `factory.py` | Creates LLM providers from model IDs | `create_model()`, `ModelConfig` |\n",
    "| `providers/` | LLM provider implementations | `GeminiLanguageModel`, `OpenAILanguageModel`, `OllamaLanguageModel` |\n",
    "| `prompting.py` | Builds few-shot prompts | `PromptTemplateStructured`, `QAPromptGenerator` |\n",
    "| `chunking.py` | Splits text into sized chunks | `TextChunk` |\n",
    "| `resolver.py` | Parses LLM output and aligns to text | `Resolver`, `WordAligner` |\n",
    "| `core/schema.py` | Structured output schemas | `BaseSchema`, `FormatModeSchema` |\n",
    "| `core/tokenizer.py` | Text tokenization | `Tokenizer`, `TokenInterval`, `TokenizedText` |\n",
    "| `io.py` | Save/load annotated documents | `save_annotated_documents()`, `load_annotated_documents_jsonl()` |\n",
    "| `visualization.py` | Interactive HTML visualization | `visualize()` |\n",
    "| `plugins.py` | Dynamic provider registration | Plugin entry points |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Setup\n",
    "\n",
    "First, import langextract and load your API key from a `.env` file.\n",
    "\n",
    "LangExtract resolves API keys from environment variables:\n",
    "- `GEMINI_API_KEY` for Gemini models\n",
    "- `OPENAI_API_KEY` for OpenAI models\n",
    "- `LANGEXTRACT_API_KEY` as a universal fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langextract as lx\n",
    "import textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Core Data Structures\n",
    "\n",
    "Before using `lx.extract()`, you need to understand the three fundamental data structures.\n",
    "\n",
    "### 2a. `Extraction` — a single extracted entity\n",
    "\n",
    "An `Extraction` represents one thing you found in the text:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Extraction:\n",
    "    extraction_class: str           # Category (e.g. \"character\", \"emotion\")\n",
    "    extraction_text: str            # The exact text span (e.g. \"ROMEO\")\n",
    "    attributes: dict | None         # Extra metadata (e.g. {\"mood\": \"happy\"})\n",
    "    char_interval: CharInterval     # Start/end positions in the source text\n",
    "    alignment_status: AlignmentStatus  # How well it matched (EXACT, FUZZY, etc.)\n",
    "```\n",
    "\n",
    "**`AlignmentStatus` values:**\n",
    "- `MATCH_EXACT` — perfect token-level match\n",
    "- `MATCH_FUZZY` — fuzzy overlap match (configurable threshold, default 0.75)\n",
    "- `MATCH_GREATER` — matched text is longer than the extraction\n",
    "- `MATCH_LESSER` — partial exact match\n",
    "\n",
    "### 2b. `ExampleData` — a few-shot training example\n",
    "\n",
    "You teach the LLM *what* to extract by providing examples:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class ExampleData:\n",
    "    text: str                        # The example text\n",
    "    extractions: list[Extraction]    # What should be extracted from it\n",
    "```\n",
    "\n",
    "### 2c. `AnnotatedDocument` — the output\n",
    "\n",
    "After extraction, you get back an `AnnotatedDocument`:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class AnnotatedDocument:\n",
    "    text: str                        # The original input text\n",
    "    extractions: list[Extraction]    # All extractions found\n",
    "    document_id: str                 # Auto-generated unique ID\n",
    "```\n",
    "\n",
    "Each extraction in the result has `char_interval` set, so you know *exactly* where in the text it was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Defining Your Extraction Task\n",
    "\n",
    "You need two things:\n",
    "1. **A prompt** — tells the LLM what kinds of entities to extract\n",
    "2. **Examples** — shows the LLM the expected output format (few-shot learning)\n",
    "\n",
    "### The Prompt\n",
    "\n",
    "The prompt describes your extraction task in natural language. Tips:\n",
    "- Be specific about what categories to extract\n",
    "- Tell it to use exact text (not paraphrase)\n",
    "- Mention that attributes should add meaningful context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract characters, emotions, and relationships in order of appearance.\n",
    "    Use exact text for extractions. Do not paraphrase or overlap entities.\n",
    "    Provide meaningful attributes for each entity to add context.\"\"\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Examples (Few-Shot Learning)\n",
    "\n",
    "Examples are **critical** — they teach the LLM:\n",
    "- What `extraction_class` values to use (e.g. \"character\", \"emotion\", \"relationship\")\n",
    "- What text spans to extract (`extraction_text` must be an exact substring)\n",
    "- What attributes to include and how to format them\n",
    "\n",
    "LangExtract uses these examples to:\n",
    "1. Build the few-shot prompt sent to the LLM\n",
    "2. Generate structured output schemas (if `use_schema_constraints=True`)\n",
    "3. Validate prompt alignment (catches mistakes early)\n",
    "\n",
    "**Important:** `extraction_text` must be an **exact substring** of the example `text`. LangExtract validates this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",\n",
    "        extractions=[\n",
    "            # Category: \"character\" — we extract the character's name\n",
    "            # extraction_text MUST appear exactly in the example text\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"ROMEO\",\n",
    "                attributes={\"emotional_state\": \"wonder\"}\n",
    "            ),\n",
    "            # Category: \"emotion\" — we extract the phrase that conveys emotion\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"But soft!\",\n",
    "                attributes={\"feeling\": \"gentle awe\"}\n",
    "            ),\n",
    "            # Category: \"relationship\" — we extract the phrase describing a relationship\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"relationship\",\n",
    "                extraction_text=\"Juliet is the sun\",\n",
    "                attributes={\"type\": \"metaphor\"}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Example text: {examples[0].text!r}\")\n",
    "print(f\"Number of extractions in example: {len(examples[0].extractions)}\")\n",
    "for ext in examples[0].extractions:\n",
    "    print(f\"  [{ext.extraction_class}] \\\"{ext.extraction_text}\\\" -> {ext.attributes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Running Extraction with `lx.extract()`\n",
    "\n",
    "This is the main entry point. Here's what each parameter does:\n",
    "\n",
    "| Parameter | Type | Default | What it does |\n",
    "|-----------|------|---------|-------------|\n",
    "| `text_or_documents` | `str`, URL, or `Iterable[Document]` | *required* | The text to extract from |\n",
    "| `prompt_description` | `str` | `None` | Natural language description of your task |\n",
    "| `examples` | `list[ExampleData]` | `None` | Few-shot examples (**required**) |\n",
    "| `model_id` | `str` | `\"gemini-2.5-flash\"` | Which LLM to use |\n",
    "| `max_char_buffer` | `int` | `1000` | Max chars per LLM call (controls chunking) |\n",
    "| `temperature` | `float` | `None` | LLM temperature (lower = more deterministic) |\n",
    "| `use_schema_constraints` | `bool` | `True` | Generate structured output schema from examples |\n",
    "| `batch_length` | `int` | `10` | Chunks per batch |\n",
    "| `max_workers` | `int` | `10` | Parallel workers for inference |\n",
    "| `extraction_passes` | `int` | `1` | Run multiple passes for better recall |\n",
    "| `format_type` | `FormatType` | `None` | Force JSON or YAML output format |\n",
    "| `debug` | `bool` | `False` | Enable debug logging |\n",
    "\n",
    "**Returns:**\n",
    "- `AnnotatedDocument` if input is a single string/URL\n",
    "- `list[AnnotatedDocument]` if input is multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"\n",
    "\n",
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",  # also supports: gpt-4o, llama3, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Result\n",
    "\n",
    "The result is an `AnnotatedDocument`. Let's look at what was extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input text: {result.text!r}\")\n",
    "print(f\"Document ID: {result.document_id}\")\n",
    "print(f\"Number of extractions: {len(result.extractions)}\")\n",
    "print()\n",
    "\n",
    "for i, ext in enumerate(result.extractions):\n",
    "    print(f\"Extraction {i+1}:\")\n",
    "    print(f\"  Class:      {ext.extraction_class}\")\n",
    "    print(f\"  Text:       \\\"{ext.extraction_text}\\\"\")\n",
    "    print(f\"  Attributes: {ext.attributes}\")\n",
    "    print(f\"  Position:   chars {ext.char_interval.start_pos}-{ext.char_interval.end_pos}\")\n",
    "    print(f\"  Alignment:  {ext.alignment_status}\")\n",
    "    # Verify the char_interval points to the right text\n",
    "    if ext.char_interval.start_pos is not None:\n",
    "        actual = result.text[ext.char_interval.start_pos:ext.char_interval.end_pos]\n",
    "        print(f\"  Verified:   \\\"{actual}\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Saving Results with `lx.io`\n",
    "\n",
    "LangExtract saves results as **JSON Lines** (`.jsonl`) — one JSON object per line, one document per line.\n",
    "\n",
    "This format is:\n",
    "- Easy to stream (read one line at a time)\n",
    "- Easy to append to\n",
    "- Compatible with most data tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a JSONL file\n",
    "lx.io.save_annotated_documents(\n",
    "    [result],                           # list of AnnotatedDocument(s)\n",
    "    output_name=\"extraction_results.jsonl\",\n",
    "    output_dir=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can load them back later\n",
    "loaded_docs = lx.io.load_annotated_documents_jsonl(\"extraction_results.jsonl\")\n",
    "print(f\"Loaded {len(loaded_docs)} document(s)\")\n",
    "print(f\"First doc has {len(loaded_docs[0].extractions)} extractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualization with `lx.visualize()`\n",
    "\n",
    "`lx.visualize()` creates an **interactive HTML widget** that:\n",
    "- Color-codes each extraction class (character = blue, emotion = green, etc.)\n",
    "- Highlights the extracted span in the text\n",
    "- Shows attributes on hover\n",
    "- Has play/pause controls to step through extractions one by one\n",
    "\n",
    "It automatically assigns colors from a 10-color palette:\n",
    "```\n",
    "Light Blue, Light Green, Light Yellow, Light Red, Light Orange,\n",
    "Light Purple, Light Teal, Light Pink, Light Grey, Pale Cyan\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `data_source`: an `AnnotatedDocument` or path to a `.jsonl` file\n",
    "- `animation_speed`: seconds between extractions during auto-play (default: 1.0)\n",
    "- `show_legend`: show color legend (default: True)\n",
    "- `gif_optimized`: larger fonts for screen recording (default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize directly from the JSONL file\n",
    "html_content = lx.visualize(\"extraction_results.jsonl\")\n",
    "\n",
    "# In Jupyter, this renders inline automatically.\n",
    "# To also save as a standalone HTML file:\n",
    "with open(\"visualization.html\", \"w\") as f:\n",
    "    if hasattr(html_content, 'data'):\n",
    "        f.write(html_content.data)\n",
    "    else:\n",
    "        f.write(html_content)\n",
    "\n",
    "html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Supported Models\n",
    "\n",
    "LangExtract supports three provider families. The `model_id` string is auto-routed to the correct provider.\n",
    "\n",
    "| Provider | Model IDs | API Key Env Var | Structured Output |\n",
    "|----------|-----------|-----------------|-------------------|\n",
    "| **Gemini** | `gemini-2.5-flash`, `gemini-2.5-pro`, etc. | `GEMINI_API_KEY` | Yes (JSON schema) |\n",
    "| **OpenAI** | `gpt-4o`, `gpt-4o-mini`, `gpt-5`, etc. | `OPENAI_API_KEY` | No |\n",
    "| **Ollama** (local) | `llama3`, `mistral`, `gemma`, `phi`, `qwen`, `deepseek`, etc. | N/A (runs locally) | No |\n",
    "\n",
    "You can also register custom providers via the **plugin system** (entry points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: A Different Example — Custom Categories\n",
    "\n",
    "Let's try a completely different extraction task to show how flexible LangExtract is. This time we'll extract **people, places, and activities** from casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract people, places, and activities mentioned in the text.\n",
    "    Use exact text for extractions. Do not paraphrase.\n",
    "    Provide meaningful attributes for each entity to add context.\"\"\")\n",
    "\n",
    "custom_examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"Sarah went to the library to study for her final exams.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"person\",\n",
    "                extraction_text=\"Sarah\",\n",
    "                attributes={\"role\": \"student\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"place\",\n",
    "                extraction_text=\"the library\",\n",
    "                attributes={\"type\": \"academic facility\"}\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"activity\",\n",
    "                extraction_text=\"study for her final exams\",\n",
    "                attributes={\"purpose\": \"exam preparation\"}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "custom_input = \"Anirudh loves to go to UIUC and he enjoys spending time with his friends and having good food\"\n",
    "\n",
    "custom_result = lx.extract(\n",
    "    text_or_documents=custom_input,\n",
    "    prompt_description=custom_prompt,\n",
    "    examples=custom_examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input: {custom_result.text!r}\")\n",
    "print()\n",
    "for ext in custom_result.extractions:\n",
    "    print(f\"  [{ext.extraction_class}] \\\"{ext.extraction_text}\\\" -> {ext.attributes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize this result directly from the AnnotatedDocument (no need to save first)\n",
    "lx.visualize(custom_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Advanced Features\n",
    "\n",
    "### 9a. Multi-pass Extraction (better recall)\n",
    "\n",
    "Sometimes the LLM misses entities on the first pass. Setting `extraction_passes > 1` runs extraction multiple times and merges the results. Overlapping extractions use a first-pass-wins strategy.\n",
    "\n",
    "```python\n",
    "result = lx.extract(\n",
    "    text_or_documents=long_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    extraction_passes=3,  # run 3 passes, merge results\n",
    ")\n",
    "```\n",
    "\n",
    "### 9b. Schema Constraints (more reliable output)\n",
    "\n",
    "When `use_schema_constraints=True` (the default), LangExtract generates a JSON schema from your examples and passes it to the LLM. This forces the model to return well-structured output. Currently best supported by Gemini.\n",
    "\n",
    "### 9c. Chunking Control\n",
    "\n",
    "`max_char_buffer` controls how much text is sent per LLM call. For long documents:\n",
    "- Smaller buffer = more API calls, but each is easier for the LLM\n",
    "- Larger buffer = fewer API calls, but may lose detail\n",
    "\n",
    "### 9d. Parallel Processing\n",
    "\n",
    "`batch_length` and `max_workers` control parallelism:\n",
    "- `batch_length`: how many chunks per batch\n",
    "- `max_workers`: max parallel API calls\n",
    "- Effective parallelism = min(batch_length, max_workers)\n",
    "\n",
    "### 9e. Processing URLs\n",
    "\n",
    "You can pass a URL directly as input — LangExtract will download and extract from it:\n",
    "\n",
    "```python\n",
    "result = lx.extract(\n",
    "    text_or_documents=\"https://example.com/article.txt\",\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")\n",
    "```\n",
    "\n",
    "### 9f. Batch Processing Multiple Documents\n",
    "\n",
    "To process many documents at once, wrap them in `Document` objects:\n",
    "\n",
    "```python\n",
    "docs = [\n",
    "    lx.data.Document(text=\"First document text...\"),\n",
    "    lx.data.Document(text=\"Second document text...\"),\n",
    "]\n",
    "\n",
    "results = lx.extract(\n",
    "    text_or_documents=docs,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")  # returns list[AnnotatedDocument]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Internals Deep Dive (for Contributors)\n",
    "\n",
    "If you want to contribute to langextract, here's how the key internal components work:\n",
    "\n",
    "### The Extraction Pipeline (step by step)\n",
    "\n",
    "**Step 1: Model Creation** (`factory.py`)\n",
    "- `factory.create_model(config)` matches `model_id` to a provider using regex patterns\n",
    "- Each provider (Gemini, OpenAI, Ollama) implements `BaseLanguageModel` with an `infer()` method\n",
    "- Schema constraints are applied via `apply_schema()` if supported\n",
    "\n",
    "**Step 2: Tokenization** (`core/tokenizer.py`)\n",
    "- Text is tokenized using a regex-based tokenizer\n",
    "- `TokenizedText` is lazy-computed and cached on the `Document` object\n",
    "- Each token has a `TokenInterval` tracking its position\n",
    "\n",
    "**Step 3: Chunking** (`chunking.py`)\n",
    "- Text is split into `TextChunk` objects, each ≤ `max_char_buffer` characters\n",
    "- Chunks include overlap to avoid splitting entities at boundaries\n",
    "- Each chunk carries its `TokenInterval` for later alignment\n",
    "\n",
    "**Step 4: Prompt Building** (`prompting.py`)\n",
    "- `PromptTemplateStructured` combines your prompt description + examples into a few-shot prompt\n",
    "- `QAPromptGenerator` formats each example as a Q&A pair\n",
    "- The chunk text is inserted as the \"question\" the LLM should answer\n",
    "\n",
    "**Step 5: Inference** (`annotation.py` + `providers/`)\n",
    "- The `Annotator` sends chunks to the LLM in parallel batches\n",
    "- Each provider's `infer()` method handles the actual API call\n",
    "- Progress bars show processing status\n",
    "\n",
    "**Step 6: Resolution** (`resolver.py`)\n",
    "- `Resolver.resolve()` parses the LLM's JSON/YAML output\n",
    "- Extracts code fences (` ```json...``` `) if present\n",
    "- Creates `Extraction` objects from the parsed data\n",
    "\n",
    "**Step 7: Alignment** (`resolver.py`)\n",
    "- `WordAligner.align()` maps each extraction back to the source text\n",
    "- Tries exact token matching first → `MATCH_EXACT`\n",
    "- Falls back to fuzzy matching if needed → `MATCH_FUZZY` (threshold 0.75)\n",
    "- Sets `char_interval` (start/end character positions) on each extraction\n",
    "- Sets `alignment_status` so you know how confident the match is\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "Key exception classes in `exceptions.py`:\n",
    "- `LangExtractError` — base class for all errors\n",
    "- `InvalidDatasetError` — empty or invalid input dataset\n",
    "- `InferenceConfigError` — model creation or config failure\n",
    "- `PromptAlignmentError` — example text doesn't contain extraction_text\n",
    "- `PromptBuilderError` — prompt template construction failure\n",
    "- `ParseError` — template parsing failure\n",
    "- `TokenUtilError` — tokenization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: How to Contribute\n",
    "\n",
    "Now that you understand the architecture, here are some areas where contributions are valuable:\n",
    "\n",
    "1. **New Providers** — Add support for more LLMs (Anthropic, Cohere, etc.) by implementing `BaseLanguageModel`\n",
    "2. **Schema Support** — Extend structured output schemas to more providers (currently best in Gemini)\n",
    "3. **Resolver Improvements** — Better parsing and alignment strategies\n",
    "4. **Visualization** — New visualization modes, better interactivity\n",
    "5. **Chunking Strategies** — Smarter text splitting (e.g., sentence-aware, paragraph-aware)\n",
    "6. **Testing** — More unit tests, edge case coverage\n",
    "7. **Documentation** — Examples, tutorials, API docs\n",
    "\n",
    "### Adding a New Provider (quick guide)\n",
    "\n",
    "1. Create a new file in `providers/` (e.g., `anthropic.py`)\n",
    "2. Implement a class that extends `BaseLanguageModel`\n",
    "3. Implement at minimum: `infer(prompt: str) -> str`\n",
    "4. Register it in the router with regex patterns for matching model IDs\n",
    "5. Or use the plugin system via entry points for external packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "import langextract as lx\n",
    "\n",
    "# 1. Define what to extract\n",
    "prompt = \"Extract characters and emotions. Use exact text.\"\n",
    "\n",
    "# 2. Provide examples\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"...\",\n",
    "        extractions=[lx.data.Extraction(extraction_class=\"...\", extraction_text=\"...\", attributes={...})]\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Extract\n",
    "result = lx.extract(text_or_documents=\"...\", prompt_description=prompt, examples=examples, model_id=\"gemini-2.5-flash\")\n",
    "\n",
    "# 4. Save\n",
    "lx.io.save_annotated_documents([result], output_name=\"output.jsonl\", output_dir=\".\")\n",
    "\n",
    "# 5. Visualize\n",
    "lx.visualize(\"output.jsonl\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
